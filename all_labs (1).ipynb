{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Twitter Scrap"
      ],
      "metadata": {
        "id": "8m50fnVt5mKU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install snscrape\n",
        "\n",
        "import pandas as pd\n",
        "import snscrape.modules.twitter as sntwitter\n",
        "import numpy as np\n",
        "import matplotlib. pyplot as plt\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import 'WordNetLemmatizer\n",
        "frun nltk.stem.porter import PorterStemmer\n",
        "\n",
        "import string\n",
        "import re\n",
        "import textblob\n",
        "from textblob import TextBlob\n",
        "\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "\n",
        "from wordcloud import ImageColorGenerator\n",
        "\n",
        "import warnings\n",
        "%matplotlib inline\n",
        "\n",
        "import os\n",
        "\n",
        "#using OS library to call CLI commands in python\n",
        "os.system(\"snscrape --json1 --max-results 10000 --since 2023-03-13 twitter-search 'chatGPT4' > text-chatGPT4-tweets.json\")\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "#create a pandas database\n",
        "tweets_df_chatGPT4 = pd.read_json('text-chatGPT4-tweets.json', lines=True)\n",
        "\n",
        "tweets_df_chatGPT4.head()\n",
        "\n",
        "#Data loading\n",
        "df_chatGPT4 = tweets_df_chatGPT4[['date', 'rawContent'.'renderedContent', 'user', 'replyCount','retweetCount','likeCount','lang','place','hashtags','viewCount']]\n",
        "print(df_chatGPT4.shape)\n",
        "\n",
        "# Twitter data cleaning, preprocessing and eda\n",
        "df2 = df_chatGPT4.drop_duplicates('renderedContent')\n",
        "print(df2.shape)\n",
        "\n",
        "df2.head()\n",
        "\n",
        "df2.info()\n",
        "\n",
        "df2.date.value_counts()\n",
        "\n",
        "# if want to convert to excel then\n",
        "# df2.to_excel('twitter_data.xlsx', index=False)\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "\n",
        "#Heat map for missing values\n",
        "plt.figure(figsize=(17,5))\n",
        "sns.heatmap(df2.isnull(), char=True, yticklabels=False)\n",
        "plt.xlabel(\"Column_Name\", size=14, weight=\"bold\")\n",
        "plt.title(\"Place of missing values in column\", fontweight=\"bold\", size=17)\n"
      ],
      "metadata": {
        "id": "tmY936Hy5rMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jK1phmiS5-RK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9gzL1Rad5-Eq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1st is theory"
      ],
      "metadata": {
        "id": "ozTv2rbBPaEe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2nd exp - Data Cleaning and Preprocessing Techniques for Enhanced Analysis and\n",
        "Modeling"
      ],
      "metadata": {
        "id": "I-CW8rmoQSeD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "import re, numpy as np, pandas as pd\n",
        "from pprint import pprint\n",
        "\n",
        "import gensim, spacy, logging, warnings\n",
        "import gensim.corpora as corpora\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models import CoherenceModel\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "stop_words =stopwords.words('english')\n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
        "\n",
        "df =pd.read_csv('google.csv')\n",
        "\n",
        "df.head(5)\n",
        "\n",
        "df=df[['Text']]\n",
        "\n",
        "df\n",
        "\n",
        "def sent_to_words(sentences):\n",
        "  for sent in sentences:\n",
        "    sent=re.sub('\\s+',' ',sent)\n",
        "    sent=re.sub(\"\\'\",\"\",sent)\n",
        "    sent=gensim.utils.simple_preprocess(str(sent), deacc=True)\n",
        "    yield(sent)\n",
        "\n",
        "All_reviews=df.Text.values.tolist()\n",
        "review_words=list(sent_to_words(All_reviews))\n",
        "print(review_words[:1])\n",
        "\n",
        "bigram=gensim.models.Phrases(review_words, min_count=5, threshold=10)\n",
        "trigram=gensim.models.Phrases(bigram[review_words], threshold=10)\n",
        "bigram_mod=gensim.models.phrases.Phraser(bigram)\n",
        "trigram_mod=gensim.models.phrases.Phraser(trigram)\n",
        "\n",
        "def process_words(texts, stop_words=stop_words, allowed_postags=['NOUN','ADJ','VERB','ADV']):\n",
        "  texts==[[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
        "  texts=[bigram_mod[doc] for doc in texts]\n",
        "  texts=[trigram_mod[bigram_mod[doc]] for doc in texts]\n",
        "  texts_out=[]\n",
        "  nlp=spacy.load('en_core_web_sm', disable=['parser','ner'])\n",
        "  for sent in texts:\n",
        "    doc=nlp(\" \".join(sent))\n",
        "    texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
        "  texts_out= [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts_out]\n",
        "  return texts_out\n",
        "\n",
        "data_final =process_words(review_words)\n",
        "\n",
        "print(data_final)\n",
        "\n",
        "\n",
        "# LDA Topic model\n",
        "id2word =  corpora.Dictionary(data_final)\n",
        "\n",
        "corpus =[id2word.doc2bow(text) for text in data_final]\n",
        "\n",
        "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=id2word, num_topics=7, random_state=100, update_every=1, chunksize=10, passes=10, alpha='symmetric', iterations =100, per_word_topics=True)\n",
        "\n",
        "pprint(lda_model.print_topics())\n",
        "\n",
        "#WORD CLOUDS OF TOP N KEYWORDS IN EACH TOPIC\n",
        "from matplotlib import pyplot as plt\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import matplotlib.colors as mcolors\n",
        "\n",
        "cols=[color for name, color in mcolors.TABLEAU_COLORS.items()]\n",
        "\n",
        "cloud=WordCloud(stopwords=stop_words, background_color='white', width=2500, height=1800, max_words=10, colormap='tab10', color_func=lambda *args, **kwargs: cols[i], prefer_horizontal=1.0)\n",
        "\n",
        "topics=lda_model.show_topics(formatted=False)\n",
        "\n",
        "fig, axes =plt.subplots(3,2, figsize=(10,10), sharex=True, sharey=True)\n",
        "\n",
        "for i, ax in enumerate(axes.flatten()):\n",
        "  fig.add_subplot(ax)\n",
        "  topic_words=dict(topics[i][1])\n",
        "  cloud.generate_from_frequencies(topic_words, max_font_size=300)\n",
        "  ax.imshow(cloud)\n",
        "  ax.set_title('Topic '+str(i),  fontdict=dict(size=16))\n",
        "  ax.axis(\"off\")\n",
        "\n",
        "plt.subplots_adjust(wspace=0, hspace=0)\n",
        "plt.axis('off')\n",
        "plt.margins(x=0, y=0)\n",
        "plt.tight_layout()\n",
        "plt.show\n",
        "\n",
        "word_dict={};\n",
        "for i in range(7):\n",
        "  words=lda_model.show_topic(i, topn=20)\n",
        "  word_dict['Topic #' + '{:02d}'.format(i)] = [i[0] for i in words]\n",
        "pd.DataFrame(word_dict)\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "from bokeh.plotting import figure, output_file, show\n",
        "from bokeh.models import Label\n",
        "from bokeh.io import output_notebook\n",
        "\n",
        "topic_weights =[]\n",
        "for i, row_list in enumerate(lda_model[corpus]):\n",
        "  topic_weights.append([w for i, w in row_list[0]])\n",
        "\n",
        "arr=pd.DataFrame(topic_weights).fillna(0).values\n",
        "\n",
        "arr=arr[np.amax(arr, axis=1)>0.35]\n",
        "\n",
        "topic_num =np.argmax(arr, axis=1)\n",
        "\n",
        "tsne_model =TSNE(n_components=2, verbose=1, random_state=0, angle=.99, init='pca')\n",
        "tsne_lda=tsne_model.fit_transform(arr)\n",
        "\n",
        "output_notebook()\n",
        "n_topics=7\n",
        "mycolors=np.array([color for name, color in mcolors.TABLEAU_COLORS.items()])\n",
        "plot = figure(title=\"t-SNE Clustering of {} LDA Topics\".format(n_topics))\n",
        "plot.scatter(x=tsne_lda[:,0], y=tsne_lda[:,1], color=mycolors[topic_num])\n",
        "\n",
        "show(plot)\n"
      ],
      "metadata": {
        "id": "6YNcYoc5QVkV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3rd exp - Data cleaning and storage: preprocess, filter and store social\n",
        "media data for business"
      ],
      "metadata": {
        "id": "1lvhpmzGPe6h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mX65L8WiPUSa"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"restaurants-dataset/swiggy.csv\")\n",
        "\n",
        "df.shape\n",
        "\n",
        "df.columns\n",
        "\n",
        "df.sample(5)\n",
        "\n",
        "df.info()\n",
        "\n",
        "df.head(3)\n",
        "\n",
        "# data cleaning - missing value handling\n",
        "#Checking number of null values in each column\n",
        "df.isnull().sum()\n",
        "\n",
        "#Checking the percentage of missing values in each column\n",
        "round(100*(df.isnull().sum()/len(df.index)), 2)\n",
        "\n",
        "# 0.6 or 0.9 are very minute, so we remove them\n",
        "df.dropna(subset=['name','rating','rating_count','cost','cuisine','address'],how='any')\n",
        "\n",
        "#Filling the null values in the lic_no with Mode based on each licence number\n",
        "m=df['lic_no'].mode()\n",
        "df.fillna(m)\n",
        "\n",
        "#Filtering the null rows from null valued columns\n",
        "df = df[-df[\"name\"].isnull()]\n",
        "df = df[-df[\"rating\"].isnull()]\n",
        "df = df[-df[\"rating_count\"].isnull()]\n",
        "df = df[-df[\"cost\"].isnull()]\n",
        "df = df[-df[\"cuisine\"].isnull()]\n",
        "df = df[-df[\"address\"].isnull()]\n",
        "df = df[-df[\"lic_no\"].isnull()]\n",
        "\n",
        "round(100*(df.isnull().sum()/len(df.index)), 2)\n",
        "\n",
        "#Now the data is cleanes without any missing values\n",
        "#Calculating the data loss\n",
        "100-round(100*len(df.index)/148541,2)\n",
        "\n",
        "#We lost 0.1% data in the data cleaning process\n",
        "\n",
        "#check for duplicated column\n",
        "df.duplicated().sum()\n",
        "\n",
        "#3. Check for appropriate datatype\n",
        "\n",
        "# # for rating column\n",
        "# replace '--' with 0 and typecasted to float\n",
        "\n",
        "def fun(column):\n",
        "    rating_int = []\n",
        "    for i in column:\n",
        "        if i=='--':\n",
        "            rating_int.append(0)\n",
        "        else:\n",
        "            rating_int.append(float(i))\n",
        "    return rating_int\n",
        "\n",
        "df['rating'] = fun(df['rating'])\n",
        "df['rating'].replace(0.0,round(df['rating'].mean(),2), inplace=True)\n",
        "\n",
        "df['rating_count'].value_counts()\n",
        "\n",
        "# for rating counts\n",
        "# convert Too Few Ratings to 1 and whole typecasted to float\n",
        "\n",
        "def fun(n):\n",
        "    try:\n",
        "        num = n.split(\" \")[0]\n",
        "        if num.split('+')[0].isdigit():\n",
        "            return float(num.split('+')[0])\n",
        "        else:\n",
        "            return 1\n",
        "    except:\n",
        "        return n\n",
        "\n",
        "df['rating_count']=df['rating_count'].apply(fun)\n",
        "\n",
        "# for cost column\n",
        "# just remove the ₹ sign and convert to float\n",
        "\n",
        "def fun(a):\n",
        "    try:\n",
        "        return float(a.split('₹')[1])\n",
        "    except:\n",
        "        return 0\n",
        "\n",
        "df['cost'] = df['cost'].apply(fun)\n",
        "\n",
        "#Data Cleaning is completed now\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4th exp - Exploratory Data Analysis in Social Media Analytics: Unveiling Insights\n",
        "from Swiggy Restaurant Data"
      ],
      "metadata": {
        "id": "HTsly3LsPh1r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# on 3rd exp above cleaning data ..do that and then eda\n",
        "# Exploratory data Analysis\n",
        "\n",
        "# Mathematical summary\n",
        "df.describe()\n",
        "\n",
        "# 1. Restaurant with Maximum Rating in Abohar\n",
        "df1 = df[df['city']=='Abohar']\n",
        "max_rating_in_city = df1.groupby('name')['rating'].max().sort_values(ascending=False).head(5)\n",
        "max_rating_in_city\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "max_rating_in_city.plot.bar()\n",
        "plt.ylabel(\"Rating in number\")\n",
        "plt.xlabel(\"Restaurent name\")\n",
        "plt.title(\"Restaurant with Maximum Rating in Abohar City of Punjab\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# 2. Number of cities (including subregions) where swiggy is having their restaurants listed?\n",
        "len(df.groupby('city'))\n",
        "\n",
        "# 3. Number of cities (don't include subregions) where swiggy is having their restaurants listed?\n",
        "df['city'].apply(lambda x: x.split(\",\")[-1]).nunique()\n",
        "\n",
        "# 4. The city with the maximum number of restaurants listed on Swiggy?\n",
        "df.groupby('city').size().sort_values(ascending=False).reset_index().iloc[0,0]\n",
        "\n",
        "# 5. Restaurant chain with maximum number of branches?\n",
        "df['name'].value_counts().sort_values(ascending=False).index[0]\n",
        "\n",
        "# 6. Top 10 cities as per the number of restaurants listed?\n",
        "top_10_cities = df.groupby('city')['name'].count().sort_values(ascending=False).head(10)\n",
        "top_10_cities\n",
        "\n",
        "plt.figure(figsize=(15, 8))\n",
        "top_10_cities.plot.bar()\n",
        "plt.ylabel(\"number of restaurants\")\n",
        "plt.xlabel(\"Cities\")\n",
        "plt.title(\"Top 10 cities as per the number of restaurants\")\n",
        "plt.show()\n",
        "\n",
        "# 7. Most Popular Cuisines served throughout the dataset?\n",
        "a = []\n",
        "for i in df['cuisine'].values:\n",
        "    try:\n",
        "        for j in i.split(','):\n",
        "            a.append(j)\n",
        "\n",
        "    except:\n",
        "        a.append(i)\n",
        "\n",
        "pd.DataFrame({\"all_cuisine\":a}).value_counts().head(1).index[0]\n",
        "\n",
        "# 8. Which city is having the least expensive restaurant in terms of cost?\n",
        "df.groupby('city')['cost'].mean().sort_values().index[0]\n",
        "\n",
        "# 9. Top 5 most popular restaurant chains in India?\n",
        "Top_5_most_popular_restaurant_chains = df.groupby('name')['rating'].mean().sort_values(ascending=False).head(5)\n",
        "Top_5_most_popular_restaurant_chains\n",
        "\n",
        "plt.figure(figsize=(15, 8))\n",
        "Top_5_most_popular_restaurant_chains.plot.bar()\n",
        "plt.ylabel(\"rating\")\n",
        "plt.xlabel(\"Restaurent\")\n",
        "plt.title(\"Top 5 most popular restaurant chains in India\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "df2 = df[df['city'].str.contains('Bangalore')]\n",
        "\n",
        "# 10. Which restaurant in Banglore has the most number of people visited(take a number of reviews for reference)?\n",
        "\n",
        "df2 = df[df['city'].str.contains('Bangalore')]\n",
        "df2.groupby('name')['rating_count'].sum().sort_values(ascending=False).index[0]"
      ],
      "metadata": {
        "id": "iqKreh3SPi8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5th exp - Develop Content( text, emoticons, image, audio, video) based  social media analytics model for business.(local business)\n",
        "(e.g.Content Based Analysis :Topic , Issue ,Trend, sentiment/opinion analysis, audio, video, image analytics)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HQ8LApRFPjUc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet --upgrade diffusers transformers accelerate\n",
        "\n",
        "!pip install torch==2.1.0\n",
        "\n",
        "!pip install -q xformers==0.0.16rc425\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import spacy\n",
        "from textblob import TextBlob\n",
        "from diffusers import StableDiffusionPipeline\n",
        "import torch\n",
        "\n",
        "nltk.download('all')\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "df=pd.read_csv('/content/Chembur_Social_cleaned_data.csv')\n",
        "\n",
        "df[\"Number of Photos\"]=df[\"Number of Photos\"].fillna('0 photos')\n",
        "df[\"Number of reviews\"]=df[\"Number of reviews\"].fillna('0 reviews')\n",
        "df[\"Name\"]=df[\"Name\"].fillna('Unknown user')\n",
        "\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "col = df['Review']\n",
        "\n",
        "words = col.apply(lambda row: nltk.word_tokenize(row))\n",
        "\n",
        "word_list = []\n",
        "for i in words:\n",
        "  for j in i:\n",
        "    word_list.append(j)\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "word_list_cleaned = []\n",
        "word_list_cleaned = [w for w in word_list if not w.lower() in stop_words and w.isalpha()]\n",
        "\n",
        "for i in range(20):\n",
        "  doc = nlp(col[i])\n",
        "  concepts = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "  for concept, label in concepts:\n",
        "    print(f\"Concept: {concept}, Label: {label}\")\n",
        "\n",
        "reviews = ' '.join(col)\n",
        "doc1 = nlp(reviews)\n",
        "trend_terms = [token.text.lower() for token in doc if token.pos_ in ['NOUN', 'PROPN', ] and not token.is_stop]\n",
        "term_freq = Counter(trend_terms)\n",
        "print(\"Top 10 Trending Terms:\")\n",
        "for term, freq in term_freq.most_common(10):\n",
        "    print(f\"{term}: {freq}\")\n",
        "\n",
        "from textblob import TextBlob\n",
        "\n",
        "topic = 'Service'  # Replace with your specific topic\n",
        "\n",
        "stance_labels = []\n",
        "\n",
        "# Check if 'Review' column exists in the DataFrame\n",
        "if 'Review' in df.columns:\n",
        "    for i in range(min(50, len(df))):\n",
        "        comment = df['Review'].iloc[i]\n",
        "        analysis = TextBlob(comment)\n",
        "\n",
        "        if topic.lower() in comment.lower():\n",
        "            if analysis.sentiment.polarity > 0:\n",
        "                stance_labels.append('Support')\n",
        "            elif analysis.sentiment.polarity < 0:\n",
        "                stance_labels.append('Against')\n",
        "            else:\n",
        "                stance_labels.append('Neutral')\n",
        "        else:\n",
        "            stance_labels.append('Not related')\n",
        "\n",
        "    # Print the results\n",
        "    for j in range(len(stance_labels)):\n",
        "        print(df['Review'].iloc[j], \" : \", stance_labels[j])\n",
        "else:\n",
        "    print(\"Error: 'Review' column not found in the DataFrame.\")\n",
        "\n",
        "\n",
        "#model_id = \"dreamlike-art/dreamlike-photoreal-2.0\"\n",
        "model_id = \"prompthero/openjourney\"\n",
        "pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
        "pipe = pipe.to(\"cuda\")\n",
        "\n",
        "prompt = ['Socials Restaurant and bar with good ambience, great service, it is also a night club with music system and DJ']\n",
        "\n",
        "images = []\n",
        "image = pipe(prompt).images[0]\n",
        "image.save('result1.jpg')\n",
        "\n"
      ],
      "metadata": {
        "id": "HoqhSa99PkZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6th exp - Develop Structure based  social media analytics model for any business.\n",
        "( e.g. Structure Based Models  -community detection,influence analysis)\n",
        "\n"
      ],
      "metadata": {
        "id": "beRsau7SPktq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "df = pd.read_table(\"/content/roadNet-PA.txt\")\n",
        "df.head()\n",
        "\n",
        "df1 = df.head(150)\n",
        "\n",
        "collab_graph = nx.from_pandas_edgelist(df1, source = 'FromNodeId', target = \"ToNodeId\")\n",
        "\n",
        "collab_graph.edges()\n",
        "\n",
        "G = nx.Graph(collab_graph)\n",
        "\n",
        "nx.draw(collab_graph, with_labels = True)\n",
        "\n",
        "nx.degree(collab_graph)\n",
        "\n",
        "print(nx.degree_centrality(collab_graph), end = \" \")\n",
        "\n",
        "betweenness_centrality = nx.betweenness_centrality(collab_graph)\n",
        "print(\"Betweenness Centrality:\", betweenness_centrality)\n",
        "closeness_centrality = nx.closeness_centrality(collab_graph)\n",
        "print(\"Closeness Centrality:\", closeness_centrality)\n",
        "\n",
        "pos = nx.spring_layout(G)\n",
        "betcent = nx.betweenness_centrality(G, normalized = True, endpoints = True)\n",
        "node_color = [20000.0 * G.degree(v) for v in G]\n",
        "node_size = [10000.0 * v for v in betcent.values()]\n",
        "plt.figure(figsize = (10,10))\n",
        "nx.draw_networkx_edges(G, pos = pos)\n",
        "plt.show()\n",
        "\n",
        "communities_gen = nx.algorithms.community.girvan_newman(collab_graph)\n",
        "top_level = next(communities_gen)\n",
        "next_level = next(communities_gen)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "pos = nx.spring_layout(collab_graph)\n",
        "for i, community in enumerate(top_level):\n",
        "  nx.draw_networkx_nodes(collab_graph, pos, nodelist=community, node_color=f\"C{i}\", label=f\"Community {i}\")\n",
        "nx.draw_networkx_edges(collab_graph, pos, alpha = 0.5)\n",
        "plt.title(\"Top Level Communities\")\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "pos = nx.spring_layout(collab_graph)\n",
        "for i, community in enumerate(next_level):\n",
        "  nx.draw_networkx_nodes(collab_graph, pos, nodelist=community, node_color=f\"C{i}\", label=f\"Community {i}\")\n",
        "nx.draw_networkx_edges(collab_graph, pos, alpha = 0.5)\n",
        "plt.title(\"Next Level Communities\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "edge_betweeness = nx.edge_betweenness_centrality(G)\n",
        "\n",
        "sorted_edges = sorted(edge_betweeness.items(), key = lambda x: x[1], reverse = True)\n",
        "\n",
        "print(\"Top bridges:\")\n",
        "for edge, betweeness in sorted_edges[:5]:\n",
        "  print(edge, \"Betweeness centrality:\", betweeness)\n",
        "\n",
        "top_bridges = sorted_edges[:5]\n",
        "\n",
        "bridges, betweeness = zip(*top_bridges)\n",
        "bridges = [str(edge) for edge in bridges]\n",
        "plt.bar(bridges, betweeness)\n",
        "plt.bar(bridges, betweeness)\n",
        "plt.xlabel('Bridge/Edge')\n",
        "plt.ylabel('Edge Betweeness Centrality')\n",
        "plt.title('Top bridges by Edge Betweeness Centrality')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "T0ZrCwgvPl9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7th exp - Develop a dashboard and reporting tool based on real time social media data. POWERBI\n",
        "\n"
      ],
      "metadata": {
        "id": "ZCgMqpDQPmhm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8th exp - DEVELOP ADVERTISING"
      ],
      "metadata": {
        "id": "hEy7jaKlPoRn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9th exp"
      ],
      "metadata": {
        "id": "xbVUwU-hPpgh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install vaderSentiment -q\n",
        "\n",
        "import re\n",
        "import sys\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "#cleaning  reviews\n",
        "def remove_pattern(input_txt, pattern):\n",
        "    r = re.findall(pattern, input_txt)\n",
        "    for i in r:\n",
        "        input_txt = re.sub(i, '', input_txt)\n",
        "    return input_txt\n",
        "\n",
        "def clean_reviews(reviews):\n",
        "    #remove twitter Return handles (RT @xxx:)\n",
        "    reviews = np.vectorize(remove_pattern)(reviews, \"RT @[\\w]*:\")\n",
        "\n",
        "    #remove twitter handles (@xxx)\n",
        "    reviews = np.vectorize(remove_pattern)(reviews, \"@[\\w]*\")\n",
        "\n",
        "    #remove URL links (httpxxx)\n",
        "    reviews = np.vectorize(remove_pattern)(reviews, \"https?://[A-Za-z0-9./]*\")\n",
        "\n",
        "    #remove special characters, numbers, punctuations (except for #)\n",
        "    reviews = np.core.defchararray.replace(reviews, \"[^a-zA-Z]\", \" \")\n",
        "\n",
        "    return reviews\n",
        "\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "def word_cloud(wd_list):\n",
        "    stopwords = set(STOPWORDS)\n",
        "    all_words = ' '.join([text for text in wd_list])\n",
        "    wordcloud = WordCloud(\n",
        "        background_color='white',\n",
        "        stopwords=stopwords,\n",
        "        width=1600,\n",
        "        height=800,\n",
        "        random_state=1,\n",
        "        colormap='jet',\n",
        "        max_words=80,\n",
        "        max_font_size=200).generate(all_words)\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    plt.axis('off')\n",
        "    plt.imshow(wordcloud, interpolation=\"bilinear\");\n",
        "\n",
        "#Subaru\n",
        "df1 = pd.read_csv('Scraped_Car_Review_subaru.csv', lineterminator='\\n')\n",
        "df1\n",
        "\n",
        "df1 = df1.dropna()\n",
        "\n",
        "#convert array to dataframe\n",
        "df = pd.DataFrame.from_dict(df1)\n",
        "df\n",
        "\n",
        "df['Review'] = clean_reviews(df['Review'])\n",
        "df\n",
        "\n",
        "scores = []\n",
        "# Declare variables for scores\n",
        "compound_list = []\n",
        "positive_list = []\n",
        "negative_list = []\n",
        "neutral_list = []\n",
        "for i in range(df['Review'].shape[0]):\n",
        "#print(analyser.polarity_scores(sentiments_pd['text'][i]))\n",
        "    if i != 491 and i != 498 and i != 511 and i != 514 and i != 517 and i != 957 and i != 958 and i != 962 and i != 970 and i != 974 and i != 976 and i != 977:\n",
        "      compound = analyzer.polarity_scores(df['Review'][i])[\"compound\"]\n",
        "      pos = analyzer.polarity_scores(df['Review'][i])[\"pos\"]\n",
        "      neu = analyzer.polarity_scores(df['Review'][i])[\"neu\"]\n",
        "      neg = analyzer.polarity_scores(df['Review'][i])[\"neg\"]\n",
        "      # print(i)\n",
        "\n",
        "      scores.append({\"Compound\": compound,\n",
        "                        \"Positive\": pos,\n",
        "                        \"Negative\": neg,\n",
        "                        \"Neutral\": neu\n",
        "                    })\n",
        "\n",
        "sentiments_score = pd.DataFrame.from_dict(scores)\n",
        "df = df.join(sentiments_score)\n",
        "df1 = df\n",
        "\n",
        "df1\n",
        "\n",
        "word_cloud(df1['Review'])\n",
        "\n",
        "#Range Rover\n",
        "df2 = pd.read_csv('/content/Scraped_Car_Review_land-rover.csv',  lineterminator='\\n')\n",
        "df2\n",
        "\n",
        "df2 = df2.dropna()\n",
        "\n",
        "#convert array to dataframe\n",
        "df = pd.DataFrame.from_dict(df2)\n",
        "df\n",
        "\n",
        "df['Review'] = clean_reviews(df['Review'])\n",
        "df\n",
        "\n",
        "scores = []\n",
        "# Declare variables for scores\n",
        "compound_list = []\n",
        "positive_list = []\n",
        "negative_list = []\n",
        "neutral_list = []\n",
        "for i in range(df['Review'].shape[0]):\n",
        "#print(analyser.polarity_scores(sentiments_pd['text'][i]))\n",
        "    # if i != 491 and i != 498 and i != 511 and i != 514 and i != 517 and i != 957 and i != 958 and i != 962 and i != 970 and i != 974 and i != 976 and i != 977:\n",
        "    compound = analyzer.polarity_scores(df['Review'][i])[\"compound\"]\n",
        "    pos = analyzer.polarity_scores(df['Review'][i])[\"pos\"]\n",
        "    neu = analyzer.polarity_scores(df['Review'][i])[\"neu\"]\n",
        "    neg = analyzer.polarity_scores(df['Review'][i])[\"neg\"]\n",
        "    # print(i)\n",
        "\n",
        "    scores.append({\"Compound\": compound,\n",
        "                      \"Positive\": pos,\n",
        "                      \"Negative\": neg,\n",
        "                      \"Neutral\": neu\n",
        "                  })\n",
        "\n",
        "sentiments_score = pd.DataFrame.from_dict(scores)\n",
        "df = df.join(sentiments_score)\n",
        "df2 = df\n",
        "\n",
        "df2\n",
        "\n",
        "word_cloud(df2['Review'])\n",
        "\n",
        "for df in [df1, df2]:\n",
        "    df['Sentiment'] = df['Review'].apply(lambda x: analyzer.polarity_scores(x))\n",
        "    df['Compound'] = df['Sentiment'].apply(lambda x: x['compound'])\n",
        "    df['Positive'] = df['Sentiment'].apply(lambda x: x['pos'])\n",
        "    df['Negative'] = df['Sentiment'].apply(lambda x: x['neg'])\n",
        "    df['Neutral'] = df['Sentiment'].apply(lambda x: x['neu'])\n",
        "\n",
        "# Calculate average scores for each DataFrame\n",
        "df1_avg_scores = df1[['Compound', 'Positive', 'Negative', 'Neutral']].mean()\n",
        "df2_avg_scores = df2[['Compound', 'Positive', 'Negative', 'Neutral']].mean()\n",
        "\n",
        "# Compare average scores between the two car brands\n",
        "print(\"Average scores for Subaru:\")\n",
        "print(df1_avg_scores)\n",
        "print(\"\\nAverage scores for Land Rover:\")\n",
        "print(df2_avg_scores)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "bar_width = 0.35\n",
        "index = range(len(df1_avg_scores))\n",
        "\n",
        "bar1 = ax.bar(index, df1_avg_scores, bar_width, label='Subaru')\n",
        "bar2 = ax.bar([i + bar_width for i in index], df2_avg_scores, bar_width, label='Land Rover')\n",
        "\n",
        "ax.set_xlabel('Sentiment')\n",
        "ax.set_ylabel('Average Score')\n",
        "ax.set_title('Average Sentiment Scores by Car Brand')\n",
        "ax.set_xticks([i + bar_width / 2 for i in index])\n",
        "ax.set_xticklabels(df1_avg_scores.index)\n",
        "ax.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8KaKBL5FXEEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **10th exp**"
      ],
      "metadata": {
        "id": "mKY15SJJPrAv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t3OzUD5rw7Oo"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "df = pd.read_csv('cleanedKFC.csv')\n",
        "\n",
        "df\n",
        "\n",
        "df.dropna(subset=['cleaned_review'], inplace=True)\n",
        "df.drop(df[df['cleaned_review'] == ''].index, inplace=True)\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "text = ' '.join(df['cleaned_review'])\n",
        "\n",
        "# Generate the word cloud\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
        "\n",
        "# Plot the word cloud\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('Word Cloud of Customer Reviews')\n",
        "plt.show()\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(df['cleaned_review'])\n",
        "\n",
        "k = 5  # Choose the number of clusters\n",
        "kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "kmeans.fit(X)\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X.toarray())\n",
        "\n",
        "df['cluster'] = kmeans.labels_\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=df['cluster'], palette='viridis', legend='full')\n",
        "plt.title('Clustering Results')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.legend(title='Cluster')\n",
        "plt.show()\n",
        "\n",
        "for cluster_num in range(k):\n",
        "    cluster_reviews = df[df['cluster'] == cluster_num]['cleaned_review'].sample(5, random_state=42)\n",
        "    print(f\"\\nCluster {cluster_num}:\")\n",
        "    for i, review in enumerate(cluster_reviews):\n",
        "        print(f\"Review {i+1}: {review}\")\n",
        "\n",
        "cluster_means = df.groupby('cluster').mean().reset_index()\n",
        "\n",
        "# Print cluster means\n",
        "print(cluster_means)\n",
        "\n",
        "num_cols = 3\n",
        "num_rows = -(-k // num_cols)  # Ceiling division to ensure enough rows\n",
        "fig, axs = plt.subplots(num_rows, num_cols, figsize=(15, 5 * num_rows))\n",
        "\n",
        "for cluster_id, ax in zip(range(k), axs.flatten()):\n",
        "    cluster_reviews_text = ' '.join(df[df['cluster'] == cluster_id]['cleaned_review'])\n",
        "    wordcloud = WordCloud(width=400, height=200, background_color='white').generate(cluster_reviews_text)\n",
        "    ax.imshow(wordcloud, interpolation='bilinear')\n",
        "    ax.set_title(f'Cluster {cluster_id}')\n",
        "    ax.axis('off')\n",
        "\n",
        "for i in range(k, num_rows * num_cols):\n",
        "    axs.flatten()[i].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Amazon review"
      ],
      "metadata": {
        "id": "IB7lrpKKKNuC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import math\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore') # Hides warning\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "warnings.filterwarnings(\"ignore\",category=UserWarning)\n",
        "sns.set_style(\"whitegrid\") # Plotting style\n",
        "np.random.seed(42) # seeding random number generator\n",
        "\n",
        "df = pd.read_csv('../input/amazon-product-reviews/amazon.csv')\n",
        "df.head()\n",
        "\n",
        "data = df.copy()\n",
        "data.describe()\n",
        "\n",
        "data.info()\n",
        "\n",
        "data[\"asins\"].unique()\n",
        "\n",
        "asins_unique = len(data[\"asins\"].unique())\n",
        "print(\"Number of Unique ASINs: \" + str(asins_unique))\n",
        "\n",
        "data[\"reviews.numHelpful\"].hist(figsize=(20,5))\n",
        "plt.show()\n",
        "\n",
        "data[\"reviews.id\"].hist( figsize=(20,5))\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(20,5))\n",
        "ax1=sns.countplot(x=\"reviews.rating\", data=data)\n",
        "for p in ax1.patches:\n",
        "    ax1.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01))\n",
        "plt.show()\n",
        "\n",
        "\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "print(\"Before : {}\".format(len(data)))\n",
        "dataAfter = data.dropna(subset=[\"reviews.rating\"])\n",
        "# Removes all NAN in reviews.rating\n",
        "print(\"After  : {}\".format(len(dataAfter)))\n",
        "dataAfter[\"reviews.rating\"] = dataAfter[\"reviews.rating\"].astype(int)\n",
        "\n",
        "split = StratifiedShuffleSplit(n_splits=10, test_size=0.2)\n",
        "for train_index, test_index in split.split(dataAfter,\n",
        "                                           dataAfter[\"reviews.rating\"]):\n",
        "    strat_train = dataAfter.reindex(train_index)\n",
        "    strat_test = dataAfter.reindex(test_index)\n",
        "\n",
        "\n",
        "print(len(strat_train))\n",
        "print(len(strat_test))\n",
        "round((strat_test[\"reviews.rating\"].value_counts()* 100/len(strat_test)),2)\n",
        "\n",
        "reviews = strat_train.copy()\n",
        "reviews.head()\n",
        "\n",
        "len(reviews[\"name\"].unique())\n",
        "\n",
        "len(reviews[\"asins\"].unique())\n",
        "\n",
        "reviews.info()\n",
        "\n",
        "reviews.groupby(\"asins\")[\"name\"].unique()\n",
        "\n",
        "different_names = reviews[reviews[\"asins\"] ==\n",
        "                          \"B00L9EPT8O,B01E6AO69U\"][\"name\"].unique()\n",
        "for name in different_names:\n",
        "    print(name)\n",
        "\n",
        "\n",
        "reviews[reviews[\"asins\"] == \"B00L9EPT8O,B01E6AO69U\"][\"name\"].value_counts()\n",
        "\n",
        "fig = plt.figure(figsize=(16,10))\n",
        "ax1 = plt.subplot(211)\n",
        "ax2 = plt.subplot(212, sharex = ax1)\n",
        "reviews[\"asins\"].value_counts().plot(kind=\"bar\", ax=ax1, title=\"ASIN Frequency\")\n",
        "np.log10(reviews[\"asins\"].value_counts()).plot(kind=\"bar\", ax=ax2,\n",
        "                                               title=\"ASIN Frequency (Log10 Adjusted)\")\n",
        "for p in ax1.patches:\n",
        "    ax1.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01))\n",
        "for p in ax2.patches:\n",
        "    ax2.annotate(str(round((p.get_height()),2)), (p.get_x() * 1.01 , p.get_height() * 1.01))\n",
        "plt.show()\n",
        "\n",
        "reviews[\"reviews.rating\"].mean()\n",
        "\n",
        "asins_count_ix = reviews[\"asins\"].value_counts().index\n",
        "fig = plt.figure(figsize=(16,5))\n",
        "ax=reviews[\"asins\"].value_counts().plot(kind=\"bar\", title=\"ASIN Frequency\")\n",
        "for p in ax.patches:\n",
        "    ax.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01))\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()\n",
        "\n",
        "fig = plt.figure(figsize=(16,5))\n",
        "sns.pointplot(x=\"asins\", y=\"reviews.rating\", order=asins_count_ix, data=reviews)\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()\n",
        "\n",
        "def sentiments(rating):\n",
        "    if (rating == 5) or (rating == 4):\n",
        "        return \"Positive\"\n",
        "    elif rating == 3:\n",
        "        return \"Neutral\"\n",
        "    elif (rating == 2) or (rating == 1):\n",
        "        return \"Negative\"\n",
        "# Add sentiments to the data\n",
        "strat_train[\"Sentiment\"] = strat_train[\"reviews.rating\"].apply(sentiments)\n",
        "strat_test[\"Sentiment\"] = strat_test[\"reviews.rating\"].apply(sentiments)\n",
        "print(strat_train[\"Sentiment\"][:15])\n",
        "\n",
        "round((strat_train[\"Sentiment\"].value_counts()*100/len(strat_train)),2)\n",
        "\n",
        "fig = plt.figure(figsize=(16,5))\n",
        "ax=strat_train[\"Sentiment\"].value_counts().plot(kind=\"bar\", title=\"Train Data Sentimental Data\")\n",
        "for p in ax.patches:\n",
        "    ax.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01))\n",
        "plt.show()\n",
        "\n",
        "round((strat_test[\"Sentiment\"].value_counts()*100/len(strat_test)),2)\n",
        "\n",
        "fig = plt.figure(figsize=(16,5))\n",
        "ax=strat_test[\"Sentiment\"].value_counts().plot(kind=\"bar\", title=\"Test Data Sentimental Data\")\n",
        "for p in ax.patches:\n",
        "    ax.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01))\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "G0olVUZvKPnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Expt 10"
      ],
      "metadata": {
        "id": "iOlzhKTaT0DU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from textblob import TextBlob\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "df = pd.read_csv('/content/The Habitat review cleaned data (1).csv')  # Update with your file path\n",
        "\n",
        "df['Review'] = df['Review'].fillna('')\n",
        "\n",
        "comments = df['Review'].tolist()\n",
        "\n",
        "def analyze_sentiment(text):\n",
        "    if pd.isna(text):  # Handle missing values\n",
        "        return 'neutral'\n",
        "    elif isinstance(text, str):  # Check if text is already a string\n",
        "        blob = TextBlob(text)\n",
        "        sentiment_score = blob.sentiment.polarity\n",
        "        return 'positive' if sentiment_score > 0 else 'negative' if sentiment_score < 0 else 'neutral'\n",
        "    else:\n",
        "        return 'neutral'\n",
        "\n",
        "df['sentiment'] = df['Review'].apply(analyze_sentiment)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(comments, df['sentiment'], test_size=0.2, random_state=42)\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_features=1000)  # Adjust max_features as needed\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "xgb = XGBClassifier()\n",
        "xgb.fit(X_train_vec, y_train_encoded)\n",
        "\n",
        "y_pred_encoded = xgb.predict(X_test_vec)\n",
        "\n",
        "y_pred = label_encoder.inverse_transform(y_pred_encoded)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "classification_rep = classification_report(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-score:\", f1)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_rep)\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, cmap='Blues', fmt='g')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "kmeans = KMeans(n_clusters=3)\n",
        "kmeans.fit(X_train_vec)\n",
        "clusters = kmeans.predict(X_test_vec)\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_test_vec.toarray())\n",
        "\n",
        "cluster_samples = [[] for _ in range(3)]\n",
        "\n",
        "for sample, cluster_label in zip(X_test, clusters):\n",
        "    cluster_samples[cluster_label].append(sample)\n",
        "\n",
        "for i, samples in enumerate(cluster_samples):\n",
        "    print(f\"Cluster {i} Samples:\")\n",
        "    for j, sample in enumerate(samples[:5]):\n",
        "        print(f\"Sample {j+1}: {sample}\")\n",
        "    print()\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "for i in range(3):\n",
        "    plt.scatter(X_pca[clusters == i, 0], X_pca[clusters == i, 1], label=f'Cluster {i}')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.title('K-Means Clustering of Reviews (PCA)')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PfXKosqBT1Xc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}